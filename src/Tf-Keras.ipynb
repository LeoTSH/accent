{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-4QJfKPmBeH",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21904,
     "status": "ok",
     "timestamp": 1568940417101,
     "user": {
      "displayName": "Leo Tay",
      "photoUrl": "",
      "userId": "17143338865945125396"
     },
     "user_tz": -480
    },
    "id": "n0skNmermLnH",
    "outputId": "4121485e-26a2-4855-f8c5-4049af014fc2"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 644,
     "status": "ok",
     "timestamp": 1568940420919,
     "user": {
      "displayName": "Leo Tay",
      "photoUrl": "",
      "userId": "17143338865945125396"
     },
     "user_tz": -480
    },
    "id": "VAK6-JRSmR9O",
    "outputId": "cc3479a7-e713-4309-de9a-6574050cd0c7"
   },
   "outputs": [],
   "source": [
    "# %cd /content/drive/My Drive/Colab Notebooks/accent/src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KMzR_jdNF33o"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T05:03:26.979689Z",
     "start_time": "2019-09-05T05:03:23.393541Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7007,
     "status": "ok",
     "timestamp": 1568940428707,
     "user": {
      "displayName": "Leo Tay",
      "photoUrl": "",
      "userId": "17143338865945125396"
     },
     "user_tz": -480
    },
    "id": "IdpwZH_SmBeI",
    "outputId": "b25a4f8c-131d-4d98-ad71-8629d6b5d155"
   },
   "outputs": [],
   "source": [
    "import re, zipfile, os, io, time, string, numpy as np, matplotlib.ticker as ticker, \\\n",
    "            matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "# from utils import process_raw, generate_input, get_max_len, process_data, convert\n",
    "\n",
    "from token_list import strip_tokens\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Bidirectional, Input, Embedding, Dense, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yrpwYNUCE1cW"
   },
   "outputs": [],
   "source": [
    "# x = ['Khoan no cua KH se den han vao 01/08/2019. So tien  2.999.898 VND, TK 12345678912. Thong tin chi tiet, lien he 1900636633. Cam on',\n",
    "# 'Khoan no cua KH se den han trong 2 ngay toi. So tien  2.999.898 VND, TK 12345678912. Bo qua neu da TT. Thong tin chi tiet, lien he 1900636633. Cam on',\n",
    "# 'Khoan no cua KH da qua han 1 ngay. TK 12345678912, so tien2.999.898 VND. Bo qua neu da TT. Thong tin chi tiet, lien he 18006288',\n",
    "# 'Khoan no cua KH da qua han 6 ngay va bi tinh phat 250,000. TK 12345678912, so tien 2.999.898VND. Vui long TT. Thong tin chi tiet, lien he 18006288',\n",
    "# 'KH da qua han 5 ky no. TK 12345678912, so tien 12.999.898VND. Vui long TT ngay lap tuc. Thong tin chi tiet, lien he 18006288',\n",
    "# 'Khoan no cua KH da bi tinh phat do lien tuc vi pham. TK12345678912, so tien 12.999.898VND. Vui long TT. Thong tin chi tiet, lien he 18006288',\n",
    "# 'Khoan no cua KH da qua han 91 ngay. Vui long TT toan bo29.999.898VND vao TK 12345678912 hom nay. Thong tin chi tiet, lien he 18006288',\n",
    "# 'Chung toi vua nhan thanh toan khoan vay cua KH tu TK12345678912. TUY NHIEN, KH van con thieu 29.999.898VND. Vui long TT toan ngay lap tuc. Cam on']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fweq4U5_G4Sb"
   },
   "source": [
    "### Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t3Yl7IcWmBeL"
   },
   "outputs": [],
   "source": [
    "def process_raw(raw_data):\n",
    "    raw_data = [seq.lower().strip() for seq in raw_data]\n",
    "\n",
    "    # Creating a space between a word and the punctuation following it\n",
    "    # Eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    raw_data = [re.sub(r\"([?.!,¿])\", r\" \\1 \", seq) for seq in raw_data]\n",
    "    raw_data = [re.sub(r'[\" \"]+', \" \", seq) for seq in raw_data]\n",
    "\n",
    "    # Replacing everything with space except (characters, \".\", \"?\", \"!\", \",\")\n",
    "    filtered_punctuations = string.punctuation\n",
    "    exclude = [',', '!', '.', '?']\n",
    "\n",
    "    for c in filtered_punctuations:\n",
    "        if c in exclude:\n",
    "            filtered_punctuations = filtered_punctuations.replace(c, '')\n",
    "\n",
    "    table = str.maketrans('', '', filtered_punctuations)\n",
    "    raw_data = [seq.translate(table) for seq in raw_data]\n",
    "    \n",
    "    # Append start and end tokens to sequences\n",
    "    processed_raw = []\n",
    "    for seq in raw_data:\n",
    "        words = seq.split()\n",
    "        words = [word.strip() for word in words]\n",
    "        processed_raw.append(' '.join(words))\n",
    "\n",
    "    return processed_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tTLjwvPomBeN"
   },
   "outputs": [],
   "source": [
    "def generate_input(processed_raw):\n",
    "    output = ''\n",
    "    for char in processed_raw:\n",
    "        if char in strip_tokens:\n",
    "            output += strip_tokens[char]\n",
    "        else:\n",
    "            output += char          \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJz8C1jjmBeP"
   },
   "outputs": [],
   "source": [
    "def get_max_len(input_data, get_index=False): \n",
    "    longest = [len(data.split()) for data in input_data]\n",
    "    if get_index:\n",
    "        print(longest.index(max(longest)))\n",
    "    return max(longest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e9ar4oQxmBeQ"
   },
   "outputs": [],
   "source": [
    "def tokenize_pad_data(data):\n",
    "    tk = Tokenizer(char_level=False, filters='')\n",
    "    tk.fit_on_texts(data)\n",
    "    data = tk.texts_to_sequences(data)\n",
    "    return data, tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Fw9xOpCmBeT"
   },
   "outputs": [],
   "source": [
    "def convert(tokenizer, tokenized_data, send_back=False):\n",
    "    original = []\n",
    "    \n",
    "    print('Tokenized Data: {}'.format(tokenized_data))\n",
    "    \n",
    "    for token in tokenized_data:\n",
    "        if token != 0:\n",
    "            if token in tokenizer.index_word:\n",
    "                original.append(tokenizer.index_word[token])\n",
    "            else:\n",
    "                original.append('<unk>')\n",
    "                    \n",
    "    print('Original Data: {}'.format(original))\n",
    "    \n",
    "    if send_back:\n",
    "        return original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7E-v-DpnNzoU"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_process_seq, max_process_seq))\n",
    "\n",
    "#     sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [input_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_process_seq, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([target_tokenizer.word_index['<s>']], 0)\n",
    "\n",
    "    for t in range(max_process_seq):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += target_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if target_tokenizer.index_word[predicted_id] == '<e>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "      \n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gSd7jEtUNzvo"
   },
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CnhFxU4ANzz_"
   },
   "outputs": [],
   "source": [
    "def restore(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iTO5vpqCHFow"
   },
   "source": [
    "### Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5142,
     "status": "ok",
     "timestamp": 1568940428712,
     "user": {
      "displayName": "Leo Tay",
      "photoUrl": "",
      "userId": "17143338865945125396"
     },
     "user_tz": -480
    },
    "id": "5rzxmuS-mBeV",
    "outputId": "d6c9284d-aad6-4759-d8ca-a7c251c92351"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-rc1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define data arguements\n",
    "# Set random seed\n",
    "np.random.seed(50)\n",
    "data_file = '../data/raw/raw_train.txt'\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZeSFzXKgHKu0"
   },
   "source": [
    "### Load and process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mpeUU7WdmBeX"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "counter = 0\n",
    "max_seq_len = 20\n",
    "no_seq = 12000\n",
    "raw_data = []\n",
    "\n",
    "# Load raw data and read first 100000 sequences with 40 or less words\n",
    "with open(data_file, 'r', encoding='utf-8') as f:    \n",
    "    while counter != no_seq:\n",
    "        line = f.readline()\n",
    "        if 5 <= len(line.split()) <= max_seq_len:\n",
    "            raw_data.append(line)\n",
    "            counter += 1\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7918,
     "status": "ok",
     "timestamp": 1568940432457,
     "user": {
      "displayName": "Leo Tay",
      "photoUrl": "",
      "userId": "17143338865945125396"
     },
     "user_tz": -480
    },
    "id": "8qXuFMzlmBeZ",
    "outputId": "35f9fb78-aa1a-427e-c7b3-d61a5368e77d"
   },
   "outputs": [],
   "source": [
    "# Shuffle raw data prior to processing\n",
    "np.random.shuffle(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Input: nhiem ky cho cac dan bieu la hai nam .\n",
      "Sample Target: <s> nhiệm kỳ cho các dân biểu là hai năm . <e>\n"
     ]
    }
   ],
   "source": [
    "# Process data\n",
    "processed_raw = process_raw(raw_data)\n",
    "\n",
    "input_data = [generate_input(seq) for seq in processed_raw]\n",
    "target_data = ['<s> ' + seq + ' <e>' for seq in processed_raw]\n",
    "\n",
    "print('Sample Input: {}'.format(input_data[0]))\n",
    "print('Sample Target: {}'.format(target_data[0]))\n",
    "\n",
    "# Free up memory\n",
    "del raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max sequence length after processing\n",
    "max_inp_len = get_max_len(input_data, get_index=False)\n",
    "max_tar_len = get_max_len(target_data, get_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate input data word tokenizer indexes\n",
    "input_tk = Tokenizer(char_level=False, filters='')\n",
    "input_tk.fit_on_texts(input_data)\n",
    "\n",
    "# Generate target data word tokenizer indexes\n",
    "target_tk = Tokenizer(char_level=False, filters='')\n",
    "target_tk.fit_on_texts(target_data)\n",
    "\n",
    "inp_vocab_size = len(input_tk.word_index)+1\n",
    "tar_vocab_size = len(target_tk.word_index)+1\n",
    "\n",
    "# Reverse vocab lookup\n",
    "input_tk_rev = {w:idx for idx, w in input_tk.word_index.items()}\n",
    "target_tk_rev = {w:idx for idx, w in target_tk.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocab size: 5552\n",
      "Target vocab size: 7823\n",
      "Max input sequence length: 30\n",
      "Max target sequence length : 32\n"
     ]
    }
   ],
   "source": [
    "print('Input vocab size: {}'.format(inp_vocab_size))\n",
    "print('Target vocab size: {}'.format(tar_vocab_size))\n",
    "print('Max input sequence length: {}'.format(max_inp_len))\n",
    "print('Max target sequence length : {}'.format(max_tar_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(np.array(input_data), np.array(target_data), test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dia ban tinh bac lieu khi do bao gom ca tinh ca mau hien nay .\n",
      "<s> địa bàn tỉnh bạc liêu khi đó bao gồm cả tỉnh cà mau hiện nay . <e>\n"
     ]
    }
   ],
   "source": [
    "# Check split\n",
    "print(x_train[-1])\n",
    "print(y_train[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10800,), (1200,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(inp, tar):\n",
    "    encoder_input_data = np.zeros((len(inp), max_inp_len), dtype='float32')\n",
    "    decoder_input_data = np.zeros((len(inp), max_tar_len), dtype='float32')\n",
    "    decoder_target_data = np.zeros((len(inp), max_tar_len, tar_vocab_size), dtype='float32')\n",
    "\n",
    "    for i, (input_seq, target_seq) in enumerate(zip(inp, tar)):\n",
    "        for t, word in enumerate(input_seq.split()):\n",
    "            encoder_input_data[i, t] = input_tk.word_index[word]\n",
    "        for t, word in enumerate(target_seq.split()):\n",
    "            if t < len(target_seq.split())-1:\n",
    "                decoder_input_data[i, t] = target_tk.word_index[word] # decoder input seq\n",
    "            if t > 0:\n",
    "                # decoder target sequence (one hot encoded)\n",
    "                # does not include the START_ token\n",
    "                # Offset by one timestep\n",
    "                decoder_target_data[i, t - 1, target_tk.word_index[word]] = 1.\n",
    "                \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = generate_data(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "['nhung', 'nguoi', 'thong', 'tri', 'da', 'dung', 'nhung', 'luan', 'diem', 'ton', 'giao', 'de', 'bien', 'minh', 'cho', 'su', 'ap', 'buc', '.']\n",
      "\n",
      "\n",
      "Decoder Input:\n",
      "['<s>', 'những', 'người', 'thống', 'trị', 'đã', 'dùng', 'những', 'luận', 'điểm', 'tôn', 'giáo', 'để', 'biện', 'minh', 'cho', 'sự', 'áp', 'bức', '.']\n",
      "\n",
      "\n",
      "Target\n",
      "['những', 'người', 'thống', 'trị', 'đã', 'dùng', 'những', 'luận', 'điểm', 'tôn', 'giáo', 'để', 'biện', 'minh', 'cho', 'sự', 'áp', 'bức', '.', '<e>']\n"
     ]
    }
   ],
   "source": [
    "print('Input:')\n",
    "print([input_tk_rev[int(w)] for w in encoder_input_data[50] if int(w) != 0])       \n",
    "print('\\n')\n",
    "\n",
    "print('Decoder Input:')\n",
    "print([target_tk_rev[int(w)] for w in decoder_input_data[50] if int(w) != 0])\n",
    "print('\\n')\n",
    "\n",
    "print('Target')\n",
    "print([target_tk_rev[np.argmax(w, axis=0)] for w in decoder_target_data[50] if np.argmax(w, axis=0) != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoder_input_data, test_decoder_input_data, test_decoder_target_data = generate_data(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "['tu', 'mot', 'giao', 'dich', 'bien', 'thanh', 'hai', 'giao', 'dich', '.']\n",
      "\n",
      "\n",
      "Decoder Input:\n",
      "['<s>', 'từ', 'một', 'giao', 'dịch', 'biến', 'thành', 'hai', 'giao', 'dịch', '.']\n",
      "\n",
      "\n",
      "Target\n",
      "['từ', 'một', 'giao', 'dịch', 'biến', 'thành', 'hai', 'giao', 'dịch', '.', '<e>']\n"
     ]
    }
   ],
   "source": [
    "print('Input:')\n",
    "print([input_tk_rev[int(w)] for w in test_encoder_input_data[50] if int(w) != 0])       \n",
    "print('\\n')\n",
    "\n",
    "print('Decoder Input:')\n",
    "print([target_tk_rev[int(w)] for w in test_decoder_input_data[50] if int(w) != 0])\n",
    "print('\\n')\n",
    "\n",
    "print('Target')\n",
    "print([target_tk_rev[np.argmax(w, axis=0)] for w in test_decoder_target_data[50] if np.argmax(w, axis=0) != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sequences: 10800\n",
      "Number of test sequences: 1200\n"
     ]
    }
   ],
   "source": [
    "print('Number of training sequences: {}'.format(len(encoder_input_data)))\n",
    "print('Number of test sequences: {}'.format(len(test_encoder_input_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10800, 30)\n",
      "(10800, 32)\n",
      "(10800, 32, 7823)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = ModelCheckpoint('./models/best.h5', save_best_only=True)\n",
    "early_stop = EarlyStopping(patience=4, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "units = 256\n",
    "embedding_dim = 256\n",
    "train_samples = len(x_train)\n",
    "test_samples = len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py:3985: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(inp_vocab_size, embedding_dim, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(tar_vocab_size, embedding_dim, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 256)    1421312     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    2002688     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 525312      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  525312      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 7823)   2010511     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 6,485,135\n",
      "Trainable params: 6,485,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile & run training\n",
    "model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8640 samples, validate on 2160 samples\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002AAB3484288> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002AAB3484288> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "8640/8640 [==============================] - 38s 4ms/sample - loss: 3.1116 - acc: 0.1252 - val_loss: 2.9358 - val_acc: 0.1472\n",
      "Epoch 2/100\n",
      "8640/8640 [==============================] - 28s 3ms/sample - loss: 2.8533 - acc: 0.1572 - val_loss: 2.8338 - val_acc: 0.1623\n",
      "Epoch 3/100\n",
      "8640/8640 [==============================] - 23s 3ms/sample - loss: 2.7375 - acc: 0.1709 - val_loss: 2.7734 - val_acc: 0.1739\n",
      "Epoch 4/100\n",
      "8640/8640 [==============================] - 23s 3ms/sample - loss: 2.6424 - acc: 0.1852 - val_loss: 2.7105 - val_acc: 0.1889\n",
      "Epoch 5/100\n",
      "8640/8640 [==============================] - 23s 3ms/sample - loss: 2.5275 - acc: 0.2085 - val_loss: 2.6153 - val_acc: 0.2223\n",
      "Epoch 6/100\n",
      "8640/8640 [==============================] - 22s 3ms/sample - loss: 2.3785 - acc: 0.2451 - val_loss: 2.4935 - val_acc: 0.2574\n",
      "Epoch 7/100\n",
      "8640/8640 [==============================] - 22s 3ms/sample - loss: 2.2182 - acc: 0.2833 - val_loss: 2.3913 - val_acc: 0.2857\n",
      "Epoch 8/100\n",
      "8640/8640 [==============================] - 22s 3ms/sample - loss: 2.0718 - acc: 0.3183 - val_loss: 2.3068 - val_acc: 0.3113\n",
      "Epoch 9/100\n",
      "8640/8640 [==============================] - 23s 3ms/sample - loss: 1.9371 - acc: 0.3461 - val_loss: 2.2432 - val_acc: 0.3277\n",
      "Epoch 10/100\n",
      "8640/8640 [==============================] - 23s 3ms/sample - loss: 1.8134 - acc: 0.3727 - val_loss: 2.1877 - val_acc: 0.3454\n",
      "Epoch 11/100\n",
      "8640/8640 [==============================] - 23s 3ms/sample - loss: 1.6989 - acc: 0.3977 - val_loss: 2.1396 - val_acc: 0.3575\n",
      "Epoch 12/100\n",
      "8640/8640 [==============================] - 23s 3ms/sample - loss: 1.5907 - acc: 0.4236 - val_loss: 2.1050 - val_acc: 0.3685\n",
      "Epoch 13/100\n",
      "8640/8640 [==============================] - 23s 3ms/sample - loss: 1.4897 - acc: 0.4488 - val_loss: 2.0713 - val_acc: 0.3763\n",
      "Epoch 14/100\n",
      "8640/8640 [==============================] - 23s 3ms/sample - loss: 1.3939 - acc: 0.4763 - val_loss: 2.0508 - val_acc: 0.3824\n",
      "Epoch 15/100\n",
      "8640/8640 [==============================] - 23s 3ms/sample - loss: 1.3031 - acc: 0.5043 - val_loss: 2.0313 - val_acc: 0.3877\n",
      "Epoch 16/100\n",
      "8640/8640 [==============================] - 23s 3ms/sample - loss: 1.2191 - acc: 0.5317 - val_loss: 2.0124 - val_acc: 0.3939\n",
      "Epoch 17/100\n",
      "8640/8640 [==============================] - 23s 3ms/sample - loss: 1.1386 - acc: 0.5569 - val_loss: 2.0018 - val_acc: 0.3981\n",
      "Epoch 18/100\n",
      "8640/8640 [==============================] - 23s 3ms/sample - loss: 1.0648 - acc: 0.5827 - val_loss: 1.9926 - val_acc: 0.4023\n",
      "Epoch 19/100\n",
      "8640/8640 [==============================] - 23s 3ms/sample - loss: 0.9952 - acc: 0.6068 - val_loss: 1.9915 - val_acc: 0.4019\n",
      "Epoch 20/100\n",
      "8640/8640 [==============================] - 23s 3ms/sample - loss: 0.9297 - acc: 0.6301 - val_loss: 1.9863 - val_acc: 0.4055\n",
      "Epoch 21/100\n",
      "8640/8640 [==============================] - 23s 3ms/sample - loss: 0.8693 - acc: 0.6521 - val_loss: 1.9843 - val_acc: 0.4094\n",
      "Epoch 22/100\n",
      "8640/8640 [==============================] - 23s 3ms/sample - loss: 0.8115 - acc: 0.6730 - val_loss: 1.9853 - val_acc: 0.4090\n",
      "Epoch 23/100\n",
      "8640/8640 [==============================] - 23s 3ms/sample - loss: 0.7577 - acc: 0.6938 - val_loss: 1.9921 - val_acc: 0.4111\n",
      "Epoch 24/100\n",
      "8640/8640 [==============================] - 23s 3ms/sample - loss: 0.7078 - acc: 0.7118 - val_loss: 1.9927 - val_acc: 0.4124\n",
      "Epoch 25/100\n",
      "8640/8640 [==============================] - 23s 3ms/sample - loss: 0.6619 - acc: 0.7300 - val_loss: 1.9982 - val_acc: 0.4129\n",
      "Epoch 00025: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Note that `decoder_target_data` needs to be one-hot encoded,\n",
    "# rather than sequences of integers like `decoder_input_data`!\n",
    "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data, \n",
    "          batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks=[model_ckpt, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(units,))\n",
    "decoder_state_input_c = Input(shape=(units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_tk.word_index['<s>']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = target_tk_rev[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '<e>' or len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "['giáp', 'có', 'các', 'đặc', 'điểm', 'sau', '<e>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' giáp có các đặc điểm sau <e>'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand = np.random.randint(1, len(x_train))\n",
    "print('Input:')\n",
    "print([target_tk_rev[np.argmax(w, axis=0)] for w in decoder_target_data[rand] if np.argmax(w, axis=0) != 0])\n",
    "\n",
    "decode_sequence(np.array([encoder_input_data[rand]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Tensorflow.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
