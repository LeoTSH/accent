{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Generate-vocabularies\" data-toc-modified-id=\"Generate-vocabularies-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Generate vocabularies</a></span></li><li><span><a href=\"#Save-dictionaries-to-file\" data-toc-modified-id=\"Save-dictionaries-to-file-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Save dictionaries to file</a></span></li><li><span><a href=\"#Tokenzize/pad-inputs-and-labels,-encode-labels\" data-toc-modified-id=\"Tokenzize/pad-inputs-and-labels,-encode-labels-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Tokenzize/pad inputs and labels, encode labels</a></span></li><li><span><a href=\"#Split-inputs-and-labels-data\" data-toc-modified-id=\"Split-inputs-and-labels-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Split inputs and labels data</a></span></li><li><span><a href=\"#Define-and-train-model\" data-toc-modified-id=\"Define-and-train-model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Define and train model</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T08:32:23.821016Z",
     "start_time": "2019-08-30T08:32:22.019261Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json, keras, numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Embedding, Dense, Input, Flatten, LSTM, Bidirectional, TimeDistributed, Activation\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "seed(1)\n",
    "set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T08:32:32.213475Z",
     "start_time": "2019-08-30T08:32:32.158012Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "processed_folder = '../data/processed/'\n",
    "# train_inputs = open('../data/train_inputs.txt').readlines()\n",
    "# # train_labels = open('../data/train_labels.txt').readlines()\n",
    "# dev_inputs = open(processed_folder+'dev_inputs.txt').readlines()\n",
    "# dev_labels = open(processed_folder+'dev_labels.txt').readlines()\n",
    "test_inputs = open(processed_folder+'test_inputs.txt').readlines()\n",
    "test_labels = open(processed_folder+'test_labels.txt').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T08:32:34.826004Z",
     "start_time": "2019-08-30T08:32:34.822800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input sentences: 30000, Number of label sentences: 30000\n"
     ]
    }
   ],
   "source": [
    "# print('Number of training sentences: {}, Number of label sentences: {}'.format(len(train_inputs), \\\n",
    "#       len(train_labels)))\n",
    "# print('Number of dev sentences: {}, Number of label sentences: {}'.format(len(dev_inputs), \\\n",
    "#       len(dev_labels)))\n",
    "print('Number of input sentences: {}, Number of label sentences: {}'.format(len(test_inputs), \\\n",
    "      len(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T08:32:39.692325Z",
     "start_time": "2019-08-30T08:32:39.354895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs vocab: 1126, Labels vocab: 69\n"
     ]
    }
   ],
   "source": [
    "inputs_data = ''.join(test_inputs)\n",
    "char_in_inputs = Counter(inputs_data)\n",
    "inputs_vocab = sorted(char_in_inputs, key=char_in_inputs.get, reverse=True)\n",
    "inputs_vocab_int = {char:idx for idx, char in enumerate(inputs_vocab, 2)}\n",
    "inputs_vocab_int['pad'] = 0\n",
    "inputs_vocab_int['unk'] = 1\n",
    "\n",
    "labels_data = ''.join(test_labels)\n",
    "char_in_labels = Counter(labels_data)\n",
    "labels_vocab = sorted(char_in_labels, key=char_in_labels.get, reverse=True)\n",
    "labels_vocab_int = {char:idx for idx, char in enumerate(labels_vocab, 2)}\n",
    "labels_vocab_int['pad'] = 0\n",
    "labels_vocab_int['unk'] = 1\n",
    "\n",
    "num_inputs_vocab = len(inputs_vocab_int)\n",
    "num_labels_vocab = len(labels_vocab_int)\n",
    "\n",
    "print('Inputs vocab: {}, Labels vocab: {}'.format(num_inputs_vocab, num_labels_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dictionaries to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T08:32:41.704918Z",
     "start_time": "2019-08-30T08:32:41.698006Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../data/processed/inputs_vocabs.json', 'w') as f:\n",
    "        json.dump(inputs_vocab_int, f, indent=4)\n",
    "        \n",
    "with open('../data/processed/labels_vocabs.json', 'w') as f:\n",
    "        json.dump(labels_vocab_int, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenzize/pad inputs and labels, encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T08:32:46.290669Z",
     "start_time": "2019-08-30T08:32:44.191456Z"
    }
   },
   "outputs": [],
   "source": [
    "max_seq_len = 256\n",
    "inputs_tokenize = []\n",
    "\n",
    "for sentence in test_inputs:\n",
    "    inputs_tokenize.append([inputs_vocab_int[char] for char in sentence])\n",
    "    \n",
    "inputs_tokenize_pad = pad_sequences(sequences=inputs_tokenize, maxlen=max_seq_len, padding='post', value=0)\n",
    "\n",
    "labels_tokenize = []\n",
    "for sentence in test_labels:\n",
    "    labels_tokenize.append([labels_vocab_int[char] for char in sentence])\n",
    "    \n",
    "labels_tokenize_pad = pad_sequences(sequences=labels_tokenize, maxlen=max_seq_len, padding='post', value=0)\n",
    "\n",
    "encode_labels = to_categorical(y=labels_tokenize_pad, num_classes=len(labels_vocab_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split inputs and labels data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T08:32:46.295756Z",
     "start_time": "2019-08-30T08:32:46.292145Z"
    }
   },
   "outputs": [],
   "source": [
    "model_inputs, model_labels = inputs_tokenize_pad[:int(-(0.2*len(inputs_tokenize_pad)))], \\\n",
    "                                encode_labels[:int(-(0.2*len(encode_labels)))]\n",
    "model_test_inputs, model_test_labels = inputs_tokenize_pad[int(-(0.2*len(inputs_tokenize_pad))):], \\\n",
    "                                        encode_labels[int(-(0.2*len(encode_labels))):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T08:33:05.334207Z",
     "start_time": "2019-08-30T08:33:05.324372Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 256)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T08:33:17.284892Z",
     "start_time": "2019-08-30T08:33:17.281512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 256, 69)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0729 17:55:18.688277 4566152640 deprecation_wrapper.py:119] From /Users/leotay/Applications/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0729 17:55:18.689251 4566152640 deprecation_wrapper.py:119] From /Users/leotay/Applications/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0729 17:55:18.692223 4566152640 deprecation_wrapper.py:119] From /Users/leotay/Applications/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0729 17:55:19.094277 4566152640 deprecation_wrapper.py:119] From /Users/leotay/Applications/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0729 17:55:19.100523 4566152640 deprecation_wrapper.py:119] From /Users/leotay/Applications/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 256, 128)          146944    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256, 256)          263168    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 256, 47)           12079     \n",
      "=================================================================\n",
      "Total params: 422,191\n",
      "Trainable params: 422,191\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "chkpt = ModelCheckpoint(filepath='../models/model.h5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "embed_input = Input(shape=(max_seq_len,))\n",
    "embedding = Embedding(input_dim=num_inputs_vocab, output_dim=128, input_length=max_seq_len, \\\n",
    "                      trainable=True)(embed_input)\n",
    "blstm = Bidirectional(LSTM(units=128, return_sequences=True))(embedding)\n",
    "output = TimeDistributed(Dense(units=num_labels_vocab, activation='softmax'))(blstm)\n",
    "model = Model(inputs=[embed_input], outputs=[output])\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0729 17:55:19.197869 4566152640 deprecation.py:323] From /Users/leotay/Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0729 17:55:20.103088 4566152640 deprecation_wrapper.py:119] From /Users/leotay/Applications/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19200 samples, validate on 4800 samples\n",
      "Epoch 1/1\n",
      "19200/19200 [==============================] - 117s 6ms/step - loss: 0.7200 - acc: 0.9004 - val_loss: 0.4275 - val_acc: 0.9276\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.42749, saving model to ../models/model.h5\n"
     ]
    }
   ],
   "source": [
    "model = model.fit(x=model_inputs, y=model_labels, batch_size=128, epochs=1, shuffle=True, \\\n",
    "                 validation_split=0.2, verbose=1, callbacks=[chkpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../models/model.h5')\n",
    "plot_model(model=model, to_file='./model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_pred = model.predict(model_test_inputs)\n",
    "out_pred = [np.argmax(x, axis=1) for x in cm_pred]\n",
    "out_pred = np.concatenate(out_pred, axis=0)\n",
    "y_ = [np.argmax(y, axis=1) for y in model_test_labels]\n",
    "y_ = np.concatenate(y_, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_matrix = confusion_matrix(y_true=y_, y_pred=out_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[772837,   1849,      0, ...,      0,      0,      0],\n",
       "       [  5003, 649726,      0, ...,      0,      0,      0],\n",
       "       [     9,  12235,      0, ...,      0,      0,      0],\n",
       "       ...,\n",
       "       [     0,    108,      0, ...,      0,      0,      0],\n",
       "       [     0,     96,      0, ...,      0,      0,      0],\n",
       "       [     0,     22,      0, ...,      0,      0,      0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leotay/Applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "class_rept = classification_report(y_true=y_, y_pred=out_pred, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.9901870092864025,\n",
       "  'recall': 0.9976132265201643,\n",
       "  'f1-score': 0.9938862461113875,\n",
       "  'support': 774686},\n",
       " '2': {'precision': 0.8600140572322607,\n",
       "  'recall': 0.992331389070893,\n",
       "  'f1-score': 0.9214468561865795,\n",
       "  'support': 654747},\n",
       " '3': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 12244},\n",
       " '4': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 9890},\n",
       " '5': {'precision': 0.14285714285714285,\n",
       "  'recall': 0.0005,\n",
       "  'f1-score': 0.000996512207274539,\n",
       "  'support': 6000},\n",
       " '6': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 4616},\n",
       " '7': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 4498},\n",
       " '8': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 4503},\n",
       " '9': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 3736},\n",
       " '10': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 4324},\n",
       " '11': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 4609},\n",
       " '12': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 4463},\n",
       " '13': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 3692},\n",
       " '14': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 3270},\n",
       " '15': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 3217},\n",
       " '16': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 3284},\n",
       " '17': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 3803},\n",
       " '18': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 3405},\n",
       " '19': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2623},\n",
       " '20': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 3068},\n",
       " '21': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2179},\n",
       " '22': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2104},\n",
       " '23': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2043},\n",
       " '24': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1703},\n",
       " '25': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1635},\n",
       " '26': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1403},\n",
       " '27': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1132},\n",
       " '28': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1319},\n",
       " '29': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 919},\n",
       " '30': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1026},\n",
       " '31': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1141},\n",
       " '32': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 787},\n",
       " '33': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 636},\n",
       " '34': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 426},\n",
       " '35': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 491},\n",
       " '36': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 430},\n",
       " '37': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 441},\n",
       " '38': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 473},\n",
       " '39': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 326},\n",
       " '40': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 123},\n",
       " '41': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 150},\n",
       " '42': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 99},\n",
       " '43': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 110},\n",
       " '44': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 108},\n",
       " '45': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 96},\n",
       " '46': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 22},\n",
       " 'accuracy': 0.9261497395833334,\n",
       " 'macro avg': {'precision': 0.04332735237773491,\n",
       "  'recall': 0.04327053512154472,\n",
       "  'f1-score': 0.04165933944576612,\n",
       "  'support': 1536000},\n",
       " 'weighted avg': {'precision': 0.8665577996509375,\n",
       "  'recall': 0.9261497395833334,\n",
       "  'f1-score': 0.8940561876796121,\n",
       "  'support': 1536000}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_rept"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
